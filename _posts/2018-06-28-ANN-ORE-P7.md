---
layout: post
title: Deep Leaning com Oracle R e Keras - Rede neural “from the scratch” (Parte 7)
subtitle: Capítulo 6 - Mãos à obra 
---

Finalmente vamos iniciar a construção de nosso modelo de rede neural. Procuramos detalhar os procedimentos ilustrando cada passo para facilitar a vida daqueles que nunca desenvolveram em R ou utilizaram o R Studio.  

Um aspecto muito interessante do R é que existem inúmeras bibliotecas disponíveis para implementar uma infinidade de funções e modelos analíticos. Talvez você use outras bibliotecas que não as escolhidas em nosso tutorial, mas sinta-se à vontade para utilizar esse código como base e alterar o que lhe for conveniente.

**Espero sinceramente que de alguma forma nosso tutorial seja útil em seus estudos ou trabalhos de Analitycs**.

**1 – Obtenção do Dataset**

Primeiramente devemos criar uma pasta para ser nossa área de trabalho. Vamos abrir um terminal e digitar os seguintes comandos:

```
cd  

mkdir –p sistemas/ann  
cd sistemas/ann  
pwd  
```

O útimo comando deverá retornar como diretório corrente /home/oracle/sistemas/ann

Em seguida, vamos obter os arquivos com os dados do nosso experimento. Para facilitar a vida de todos, transferi do Kaggle os arquivos para uma área em meu Github, de forma que para obter os referidos arquivos, execute:

```
curl -O https://raw.githubusercontent.com/wilson-camargo-jr/ORE-ANN/master/data/test.csv  

curl -O https://raw.githubusercontent.com/wilson-camargo-jr/ORE-ANN/master/data/train.csv  

ls -l  
```

A pasta ann deverá conter os seguintes arquivos:

test.csv  
train.csv   

Agora vamos acionar o R Studio.

```
rstudio
```

A interface do R Studio será exibida conforme figura abaixo:
 
![rsutdio](https://wilson-camargo-jr.github.io/img/console-r.jpg)  

Primeiramente, vamos ajustar o R Studio para trabalhar em nossa recém criada área de trabalho, utilizando a aba “Files” e em seguida configurando nossa nova pasta como o Diretório de Trabalho, acionando o ícone “More” e selecionando a opção “Set As Working Directory”, conforme figura abaixo:
  
![rsutdio](https://wilson-camargo-jr.github.io/img/console-r-wd.jpg)  

Agora vamos criar o arquivo fonte para nosso aplicativo de Redes Neurais. Na interface do R Studio, clique no ícone  “+” e selecione R Script, conforme figura abaixo:

![rsutdio](https://wilson-camargo-jr.github.io/img/console-r-nf.jpg)  

Teremos então um novo arquivo criado para edição conforme abaixo:

![rsutdio](https://wilson-camargo-jr.github.io/img/console-r-nf2.jpg)  

**Você poderá obter o código completo de nosso programa em:**
```
curl -O https://raw.githubusercontent.com/wilson-camargo-jr/ORE-ANN/master/ann.r
```

Em seguida insira os comentários abaixo no **editor de programas** do R Studio. O editor de programas fica localizado no canto superior esquerdo de sua tela.

```
# ANN com Keras para Previsao de Pagamentos de Emprestimos Bancarios. 
# Ultima modificacao - Wilson - 03/07/2018
#
```

Agora vamos salvar esse arquivo com o nome ann.r clicando no ícone indicado, conforme figura abaixo:
 
![rsutdio](https://wilson-camargo-jr.github.io/img/console-r-sv2.jpg)  

Na caixa de diálogo que surgirá coloque o nome do arquivo “ann.r” e selecione “save”.

![rsutdio](https://wilson-camargo-jr.github.io/img/console-r-sv.jpg)  


**2 – Obtenção e Preparação dos dados**

As fases mais longas e trabalhosas das atividades de um cientista de dados são as de obtenção e preparação dos dados.

Na primeira, é necessário localizar os dados e montar todos os processos administrativos e operacionais para obter, armazenar e disponibilizar as informações necessárias para as etapas seguintes.

Na fase de preparação é preciso descobrir falhas ocorridas na obtenção dos dados, valores ausentes, tratamento de exceções, valores com erros de digitação, dados repetidos, etc., e a melhor forma de descobrir o que fazer é examinando os dados, e é o que faremos aogra.

Nosso dataset (aqui simplificamos a fase de obtenção) está disposto em 2 arquivos que obtivemos por download, mas como nosso propósito é mostrar como trabalhamos com dados armazenados em banco de dados, então nossa primeira tarefa aqui será jogar os dados para tabelas no banco de dados. Isso é feito de forma muito simples na própria **console** do R Studio, com os comandos abaixo.

A console do R Studio, fica localizada no canto inferior esquerdo de sua tela e pode ser redimensionada conforme apropriado para melhor visualização. 

```
library(ORE)
ore.connect("RQUSER", password="oracle", conn_string="", all=TRUE)
ore.create(read.csv('train.csv'), table = "train") 
ore.create(read.csv('test.csv'), table = "test")

ore.ls()
```

![rsutdio](https://wilson-camargo-jr.github.io/img/create-tables.jpg)

Agora existem duas tabelas no banco de dados com nomes test e train, que em um caso real, seriam tabelas de trabalho de sistemas que poderiam ser diretamente utilizadas via R para processos de Analytics.
 
Olhe que trabalho com bancos de dados há muito tempo, mas confesso que me surpreendi na primeira vez que fiz esse tipo de carga de dados.... quem já teve que fazer imports, export, dump de dados para o banco sabe bem do que estou falando.... 

Com os dados carregados no banco, vamos iniciar nossas análises. 

Aliás, só mais uma etapa de preparação. Vamos carregar as bibliotecas que utilizaremos em nosso programa. Para isso execute os seguintes comandos na **console** do R Studio.

```
install.packages("caret")    
install.packages("DMwR")  
install.packages("caTools")
install.packages("lattice")  
install.packages("ggplot2")  
install.packages("grid")  
```

Certo. Bibliotecas carregadas, dados também carregados. 3, 2, 1... Já. 

Um aspecto interessante do ORE, é que as tabelas que vimos quando executamos o comando ore.ls(), já estão disponíveis para execução dos comandos R sem que tenhamos que transferi-las para variáveis locais. Experimente os seguintes comandos na **console** do R Studio: 

```
class(train)
```

Perceba que já temos um objeto R do tipo ore.frame pronto para utilização sem termos carregado nenhum dado para a memória local.

Com o comando names, podemos ver colunas da tabela que criamos na carga dos dados.

``` 
names(train)
```

Número de linhas e coluna do nosso dataset train.

```
dim(train)
```

Qua tal dar uma olhada nos dados do nosso dataset ?

```
head(train)
```

![rsutdio](https://wilson-camargo-jr.github.io/img/data-exam.jpg)  

Esses são os 6 primeiros registros de nosso dataset, e ao observar com atenção, já vemos um problema. Perceba que nos registros 2 e 6 da coluna “Credit_History” encontramos um NA, ou seja, "missing value" - não existe a informação no dataset.

Isso pode ser um problema dependendo do algoritmo utilizado, pois alguns deles não lidam bem com “falta de informação”.

Vamos então examinar as outras colunas, entender a real dimensão do problema e então escolher o melhor tratamento para os dados ausentes. Para isso vamos utilizar o comando summary.

```
summary(train)
```

![rsutdio](https://wilson-camargo-jr.github.io/img/null-values.jpg)  

Vemos que existem diversas colunas com ausência de informação em seus registros, por exemplo, na coluna Gender, temos 489 registros com valores _“Male”_, 112  com _“Female”_ e 13 com indicação de informações ausentes.

Temos então que decidir o que fazer com os valores ausentes em cada coluna. Poderíamos simplesmente excluir as linhas que possuem colunas NA, ou preencher os ausentes com algum valor que seja mais apropriado, como por exemplo, a média aritmética das observações da coluna, o valor mais frequente, etc. 

A análise dever ser feita caso a caso, lembrando sempre que em qualquer das hipóteses de remoção ou substituição existe a possibilidade de estarmos inserindo algum componente de erro em nosso modelo.

**a) Lidando com valores não disponíveis (NA)**

Vamos carregar nossos datasets “train” e “test” em variáveis locais para realizar os devidos ajustes. Insira o seguinte código no **editor de programas** e execute os comandos:

```
# Carregando a biblioteca ORE  
library(ORE)  

# Conectando ao banco de dados.   
ore.connect("RQUSER", password="oracle", conn_string="", all=TRUE)   

# Carregando tabela para variavel de trabalho local  
training_set <- ore.pull(train)  
predict_set  <- ore.pull(test)  
```

Para executar o código no editor marque as linhas a serem executadas e clique em **Run** conforme figura abaixo.

![rsutdio](https://wilson-camargo-jr.github.io/img/run-code.jpg)

Ignore as mensagens de warning quando referentes a unique key:

\> # Carregando a biblioteca ORE   
\> library(ORE)   
\>    
\> # Conectando ao banco de dados.   
\> ore.connect("RQUSER", password="oracle", conn_string="", all=TRUE)   
\>    
\> # Carregando tabela para variavel de trabalho local   
\> training_set <- ore.pull(train)   
<span style="color:red">Warning message: ORE object has no unique key - using random order</span>  
\> predict_set  <- ore.pull(test)  
<span style="color:red">Warning message: ORE object has no unique key - using random order</span>  
\>  

Vamos rever quais colunas apresentam valores nulos em seus registros.  Execute o seguinte comando na **console** do R studio.

```
colSums(is.na(training_set))
colSums(is.na(predict_set))
```

![rsutdio](https://wilson-camargo-jr.github.io/img/null-values2.jpg)
 
 
Nossa estratégia (puramente para fins didáticos) será a seguinte:

1)	Gender:		substituir “NA” por “Male”  
2)	Married:		substituir “NA” por “Yes”  
3)	Dependents:		substituir “NA” por “0”  
4)	Self_employed: 	substituir “NA” por “No”  
5)	LoanAmount: 	substituir “NA” pela média aritmética dos valores  
6)	LoanAmount_Term:  substituir “NA” pela mediana dos valores  
7)	Credit_History:	substituir “NA” por 1  

Então vamos acrescentar ao nosso código no **editor de programas** as seguintes linhas:  

```
# Ajustando “NA” no training_set      
#     
# Trocando valores nulos da coluna Gender por "Male".   
training_set$Gender[is.na(training_set$Gender)] <- "Male"    

# Trocando valores nulos da coluna Married por "Yes".   
training_set$Married[is.na(training_set$Married)] <- "Yes"    

# Trocando valores nulos da coluna Dependents por "0".   
training_set$Dependents[is.na(training_set$Dependents)] <- "0"    

# Trocando valores nulos da coluna Self_Employes por "No".   
training_set$Self_Employed[is.na(training_set$Self_Employed)] <- "No"    

# Trocando valores nulos da coluna LoanAmount pela media dos valores.   
training_set$LoanAmount <- ifelse(is.na(training_set$LoanAmount), mean(training_set$LoanAmount, na.rm = TRUE), training_set$LoanAmount)    

# Trocando valores nulos da coluna Loan_Amount_Term pela mediana dos valores.   
training_set$Loan_Amount_Term <- ifelse(is.na(training_set$Loan_Amount_Term), median(training_set$Loan_Amount_Term,na.rm = TRUE), training_set$Loan_Amount_Term)    

# Trocando valores nulos da coluna Credit_History por 1.   
training_set$Credit_History[is.na(training_set$Credit_History)] <- 1    
```

Então vamos repetir o mesmo procedimento para o dataset que nomeamos “predict_set”.  Assim, insira o seguinte código no **editor de programas** e execute.

```
# Ajustando “NA” no predict_set    
#   
# Trocando valores nulos da coluna Gender por "Male".   
predict_set$Gender[is.na(predict_set$Gender)] <- "Male"    

# Trocando valores nulos da coluna Married por "Yes".   
predict_set$Married[is.na(predict_set$Married)] <- "Yes"    

# Trocando valores nulos da coluna Dependents por "0".   
predict_set$Dependents[is.na(predict_set$Dependents)] <- "0"    

# Trocando valores nulos da coluna Self_Employes por "No".   
predict_set$Self_Employed[is.na(predict_set$Self_Employed)] <- "No"    

# Trocando valores nulos da coluna LoanAmount pela media dos valores.   
predict_set$LoanAmount <- ifelse(is.na(predict_set$LoanAmount), mean(predict_set$LoanAmount, na.rm = TRUE), predict_set$LoanAmount)    

# Trocando valores nulos da coluna Loan_Amount_Term pela mediana dos valores.   
predict_set$Loan_Amount_Term <- ifelse(is.na(predict_set$Loan_Amount_Term), median(predict_set$Loan_Amount_Term, na.rm = TRUE), predict_set$Loan_Amount_Term)    

# Trocando valores nulos da coluna Credit_History por 1.   
predict_set$Credit_History[is.na(predict_set$Credit_History)] <- 1   
```

**Salve seu arquivo.**

Vamos verificar o resultado dos ajustes realizados executando os seguintes comandos na **console** do R Studio:

```
colSums(is.na(training_set))  
colSums(is.na(predict_set))  
```

![rsutdio](https://wilson-camargo-jr.github.io/img/null-values3.jpg)

**b) Convertendo as variáveis categóricas**

Em nosso dataset temos diversas variáveis não numéricas, dentre elas “Gender”,  “Married”,  “Dependents”, etc. 

Para poderem servir de insumo no modelo ANN, essas variáveis precisam ser convertidas para uma notação binária, de modo a não causar incoerências nem tendências no modelo. Além disso vamos normalizar os valores numéricos para evitar que as dimensões provoquem anomalias em nosso modelo.

Para isso insira os seguintes comandos no **editor de programas**:
``` 
#   
# Codificando as colunas com fatores do training_set (usando as biblioteca caret e DMwR):   
#  
library(caret)  
library(DMwR)   
#  
# Loan_Status = Y significa que o cliente pagou o emprestimo   
#   
dmy_train <- dummyVars(~ Gender + Married + Dependents + Education + Self_Employed + Credit_History + Property_Area + Loan_Status, data = training_set)    

dmy_train_set <- data.frame(predict(dmy_train, newdata = training_set))    

# Removendo as colunas codificadas redundantes para evitar a "Armadilha da Variavel Dummy"  
 
dmy_train_red <- dmy_train_set[c(1,3,5,6,7,9,11,13,14,15,17)]    
#   
# Removendo as colunas nao utilizadas (redundantes e desnecessarias) do training_set   
# 
val_train_set <- training_set[c(7,8,9,10)]    

# Ajustando a escala e centrando ao redor da media   
val_train_centered <- scale(val_train_set, center = TRUE, scale = TRUE)    

# Unindo os datasets de treino formando o dataset de aprendizado   
learn_set <- cbind(val_train_centered, dmy_train_red)     

#   
# Executando o mesmo procedimento para o predict_set   
#   
dmy_predict <- dummyVars(~ Gender + Married + Dependents + Education + Self_Employed + Credit_History + Property_Area, data = predict_set)    
dmy_predict_set <- data.frame(predict(dmy_predict, newdata = predict_set))   
dmy_predict_red <- dmy_predict_set[c(1,3,5,6,7,9,11,13,14,15)]   
val_predict_set <- predict_set[c(7,8,9,10)]   
val_predict_centered <- scale(val_predict_set, center = TRUE, scale = TRUE)   

# Unindo os datasets de previsão  
 predict <- cbind(val_predict_centered,dmy_predict_red)   
```

Finalmente vamos dividir nosso dataset com uma parte treino e outra para teste de nosso modelo.

```
# Dividindo o dataset de aprendizado em dataset de treino e teste   
library(caTools)   

set.seed(123)   
split <- sample.split(learn_set$Loan_Status.N, SplitRatio = 0.80)   

train_set <- subset(learn_set, split == TRUE)   
test_set  <- subset(learn_set, split == FALSE)    

x_train_set <- train_set[,-15]   
y_train_set <- train_set[,15]    
x_test_set <- test_set[,-15]   
y_test_set <- test_set[,15]    

x_train_set <- as.matrix(x_train_set, nrow=c(nrow(x_train_set)), ncol=15)   
x_test_set  <- as.matrix(x_test_set, nrow=c(nrow(x_test_set)), ncol=15)   
``` 

**Salve seu arquivo.**

Programando a rede neural com Keras

Primeiramente vamos criar nosso modelo de rede neural instanciando um objeto sequencial.

```
# Modelando a Rede Neural
# Carregando a biblioteca Keras
library(keras)

# Inicializando o modelo
classificador <- keras_model_sequential()
```

O próximo passo é adicionar as camadas da rede neural, com os vários tipos de camadas: dense, activation, dropout.

Quanto mais camadas, mais profunda é a rede. Cada camada tem a função de criar novos atributos mais complexos.  

```
# Adicionando as camadas.
classificador %>%
  layer_dense(units = 14, kernel_initializer="glorot_uniform", input_shape = c(14)) %>% 
  layer_activation('relu') %>%
  layer_dropout(0.2) %>%
  layer_dense(units = 14, kernel_initializer="glorot_uniform") %>% 
  layer_activation('relu') %>%
  layer_dropout(0.2) %>%
  layer_dense(units =  1, kernel_initializer="glorot_uniform") %>% 
  layer_activation('sigmoid')

# Apresentando o resumo do modelo
summary(classificador)
```

**Salve seu arquivo.**

Nosso modelo é composto por uma camada de entrada, duas camadas escondidas e uma camada de saída.

É na camada "densa" que é definida a estrutura, tamanho de entradas e saídas, o tipo de inicialiaçao dos pesos e implementada a operação de cálculo do pulso a ser enviado para a proxima camada, em uma rede neueal.

A camada de ativação, ou função de ativação, no nosso modelo a _relu_, é quem determina o tipo de transformação a ser implementado para geração do pulso.

A camada de dropout ajuda evitar o overfitting no modelo.

Se quiser experimentar, crie ou remova camadas utilizando dimensões diferentes, use outros tipos de ativação, modifique ou retire a camada de dropout para ver variações no resultado de seu modelo.

O passo final de definição do modelo é a compilação, onde keras utiliza o Tensorflow como back-end para construir o modelo.
Nesse ponto, keras precisa de dois importantes inputs, a função de perda e o algoritimo de otimização.  

```
# Compilando o modelo com a funcao de perda crossentropy e otimizador adam
classificador %>% compile(loss = 'binary_crossentropy', 
                          optimizer = optimizer_adam(), 
                          metrics = c('accuracy'))
```

Agora estamos com o modelo de rede neural pronto. Usamos a função **fit** para treinar o modelo passando o dataset de treino e o resultado esperado. Quando terminar o treino, keras informará a acuracia do modelo.  

Os algorítimos de otimização, em nosso modelo o _adam_, servem para minimizar (ou maximizar) a função objetivo (que é outro nome da função Erro E(x) de nosso modelo) que é simplesmente uma função matemática dependente dos paramêtros de aprendizagem do modêlo. Por exemplo: os pesos e bias são os parâmetros de aprendizagem das redes neurais, e são usados no cálculo do pulso de saída, sendo a cada iteração treinados e atualizados para obtenção da solução ótima, que nesse caso é minimizar a função de perda (loss), em nosso modelo _binary_crossentropy_.  

Normalmente, quanto menor o resultado da função de perda melhor o é modelo, exceto quando este estiver "overfitted".  

Como variação para o modelo, aqui podemos trocar a função de perda e o otimizador.

```
# Treinando o modelo
history <- classificador %>% fit(x_train_set, y_train_set, epochs = 100, batch_size = 50,  validation_split = 0.25)
```

Depois de treinar o modelo, passamos para a fase de teste. o teste é realizado utilizando a função **evaluate** passando o dataset de teste e o resultado esperado.

Podemos obter alternativas ao nosso modelo alterando o numero de epochs (vezes que o modelo será treinado), batch_size (número de amostras processadas para fazer o ajuste dos pesos), e validation_split (percentual do dataset a ser utilizado para o cálculo da função loss durante o treino).  

```
# Testando o modelo
taxa_erro_teste <- classificador %>% evaluate(x_test_set, y_test_set)
y_pred <- classificador %>% predict_classes(x_test_set)
```

**Salve seu arquivo.**

Agora calcularemos algumas métricas além da acuracia do modelo.

A Confusion Matrix nos dá a quantidade de acertos e erros Tipo I e Tipo II da previsão de nosso modelo.

```
# Calculando a Confusion Matrix

cm <- table(y_test_set, y_pred)

# Analises de resultado
Test_TP <- cm[1,1]
Test_TN <- cm[2,2]
Test_FP <- cm[1,2]
Test_FN <- cm[2,1]

TestAccuracy    <- (Test_TP+Test_TN)/(Test_TP+Test_TN+Test_FP+Test_FN)
TestPrecision   <- Test_TP/(Test_TP+Test_FP) 
TestRecall      <- Test_TP/(Test_TP+Test_FN) # Sensivity
TestSpecificity <- Test_TN/(Test_TN+Test_FP)
```

Agora com o modelo treinado e testado podemos realizar previsões no dataset para previsão.

```
# Utilizando o modelo com o dataset de previsao.
predict <- as.matrix(predict, nrow=c(nrow(predict)), ncol=15)
predictions <- classificador %>% predict_classes(predict)
histogram(predictions)
```

**Salve seu arquivo.**

Terminamos.

Agora temos todos os insumos para preparar uma apresentação para a área de negócios de nosso banco, com as respostas que motivaram nosso processo de analytics.

Mas durante essa processo de preparação, alguns pontos devem ser analisados.

Apesar de termos uma taxa de acuracia alta, por volta de 80%, nossa amostra é relativamente pequena, o que pode ter provocado um overfitting em nosso modelo.

Execute os seguintes comandos no **console** do R Studio:

```
TestAccuracy  
TestPrecision   
TestRecall  
TestSpecificity  
```

Você consegue encontrar outros pontos interessantes em nosso modelo ?


Bem, esse é o final de nosso tutorial, espero que tenham aproveitado e encontrado pontos de inspiração para seus trabalhos e estudos, e principalmente visto a utilidade do Oracle R Enterprise para Analytics.

Você pode encontrar mais sobre o ORE em:

<sub><a href="https://docs.oracle.com/cd/E83411_01/OREUG/toc.htm" target="_blank">https://docs.oracle.com/cd/E83411_01/OREUG/toc.htm</a></sub>

<div id="disqus_thread"></div>
<script>
    
    
    var disqus_config = function () {
        // Replace PAGE_URL with your page's canonical URL variable
        this.page.url = 'https://wilson-camargo-jr.github.io/2018-06-28-ANN-ORE-P7';  
        
        // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        this.page.identifier = '2018-06-28-ANN-ORE-P7'; 
    };
    

    
    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
        var d = document, s = d.createElement('script');
        
        // IMPORTANT: Replace EXAMPLE with your forum shortname!
        s.src = 'https://wilson-camargo-jr.disqus.com/embed.js';
        
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>
    Please enable JavaScript to view the 
    <a href="https://disqus.com/?ref_noscript" rel="nofollow">
        comments powered by Disqus.
    </a>
</noscript>
