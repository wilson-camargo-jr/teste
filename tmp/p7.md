---
layout: post
title: Deep Leaning com Oracle R e Keras - Rede neural “from the scratch” (Parte 7)
subtitle: Capítulo 6 - Mãos à obra 
---

Finalmente vamos iniciar a construção de nosso modelo de rede neural. Procuramos detalhar os procedimentos ilustrando cada passo para facilitar a vida daqueles que nunca desenvolveram em R ou utilizaram o R Studio.

**1 – Obtenção do Dataset**

Primeiramente devemos criar uma pasta para ser nossa área de trabalho. Vamos abrir um terminal e digitar os seguintes comandos:

```
cd  

mkdir –p sistemas/ann  
cd sistemas/ann  
pwd  
```

O útimo comando deverá retornar como diretório corrente /home/oracle/sistemas/ann

Em seguida, vamos obter os arquivos com os dados do nosso experimento. Para facilitar a vida de todos, transferi do  Kaggle os arquivos para uma área em meu Github, de forma que para obter os referidos arquivos, execute:

```
curl -O https://raw.githubusercontent.com/wilson-camargo-jr/ORE-ANN/master/data/test.csv

curl -O https://raw.githubusercontent.com/wilson-camargo-jr/ORE-ANN/master/data/train.csv

ls -l
```

A pasta ann deverá conter os seguintes arquivos:

test.csv  
train.csv   

Agora vamos acionar o R Studio.

```
rstudio
```

A interface do R Studio será exibida conforme figura abaixo:
 
![rsutdio](https://wilson-camargo-jr.github.io/img/console-r.jpg)  

Primeiramente, vamos ajustar o R Studio para trabalhar em nossa recém criada área de trabalho, utilizando a aba “Files” e seguida configurando nossa nova pasta como o Diretório de Trabalho, acionando o ícone “More” e selecionando a opção “Set As Working Directory”, conforme figura abaixo:
  
![rsutdio](https://wilson-camargo-jr.github.io/img/console-r-wd.jpg)  

Agora vamos criar o arquivo fonte para nosso aplicativo de Redes Neurais.  Na interface do R Studio,  clique no ícone  “+” e selecione R Script, conforme figura abaixo:

![rsutdio](https://wilson-camargo-jr.github.io/img/console-r-nf.jpg)  


Teremos então um novo arquivo criado para edição conforme abaixo:

![rsutdio](https://wilson-camargo-jr.github.io/img/console-r-nf2.jpg)  

Em seguida insira os comentários abaixo no **editor de programas** do R Studio. O editor de programas fica localizado no canto superior esquerdo de sua tela.

```
# ANN com Keras para Previsao de Pagamentos de Emprestimos Bancarios. 
# Ultima modificacao - Wilson - 03/07/2018
#
```

Agora vamos salvar esse arquivo com o nome ann.r clicando no ícone indicado, conforme figura abaixo:
 
![rsutdio](https://wilson-camargo-jr.github.io/img/console-r-sv2.jpg)  

Na caixa de diálogo que surgirá coloque o nome do arquivo “ann.r” e selecione “save”.

![rsutdio](https://wilson-camargo-jr.github.io/img/console-r-sv.jpg)  





**2 – Obtenção e Preparação dos dados**

As fases mais longas e trabalhosas das atividades de um cientista de dados são de obtenção e preparação dos dados.

Na primeira, é necessário localizar os dados e montar todos os processos administrativos e operacionais para obter, armazenar e disponibilizar as informações necessárias para as etapas seguintes.

Na fase de preparação é preciso descobrir falhas ocorridas na obtenção dos dados, valores ausentes, tratamento de exceções, valores com erros de digitação, dados repetidos, etc., e a melhor forma de descobrir o que fazer é examinando os dados, e é o que faremos aogra.

Nosso dataset (aqui simplificamos a fase de obtenção) está disposto em 2 arquivos que obtivemos por download, mas como nosso propósito é mostrar como trabalhamos com dados armazenados em banco de dados, então nossa primeira tarefa aqui será jogar os dados para tabelas no banco de dados. Isso é feito de forma muito simples na própria **console** do R Studio, com os comandos abaixo.

A console do R Studio, fica localizada no canto inferior esquerdo de sua tela e pode ser redimensionado conforme apropriado para melhor visualização. 

```
library(ORE)
ore.connect("RQUSER", password="oracle", conn_string="", all=TRUE)
ore.create(read.csv('train.csv'), table = "train") 
ore.create(read.csv('test.csv'), table = "test")

ore.ls()
```

![rsutdio](https://wilson-camargo-jr.github.io/img/create-tables.jpg)

Agora existem duas tabelas no banco de dados com nomes test e train, que em um caso real, seriam tabelas de trabalho de sistemas que poderiam ser diretamente utilizadas com Analytics.
 
Trabalho com bancos de dados há muito tempo, mas confesso que me surpreendi na primeira vez que fiz esse tipo de carga de dados.... quem já teve que fazer imports de dados para o banco sabe bem do que estou falando.... 

Com os dados carregados no banco, vamos iniciar nossas análises. 

Um aspecto interessante do ORE, é que as tabelas que vimos quando executamos o comando ore.ls(), já estão disponíveis para execução dos comandos R sem que tenhamos que carrega-las em variáveis locais. Experimente os seguintes comandos na console do R Studio: 

```
class(train)
```

Perceba que já temos um objeto R do tipo ore.frame pronto para utilização sem termos carregado nenhum dado para a memória local.

Com o comando names, podemos ver colunas da tabela que criamos na carga dos dados.

``` 
names(train)
```

Número de linhas e coluna do nosso dataset train.

```
dim(train)
```

Qua tal dar uma olhada nos dados do nosso dataset ?

```
head(train)
```

![rsutdio](https://wilson-camargo-jr.github.io/img/data-exam.jpg)  

Esses são os 6 primeiros registros de nosso dataset, e ao observar com atenção, já vemos um problema. Perceba que nos registros 2 e 6 da coluna “Credit_History” encontramos um NA, ou seja, "missing value" - não existe a informação no dataset.

Isso pode ser um problema dependendo do algoritmo utilizado, pois alguns deles não lidam bem com “falta de informação”.

Vamos então examinar as outras colunas, entender a real dimensão do problema e então escolher o melhor tratamento para os dados ausentes. Para isso vamos utilizar o comando summary.

summary(train)

![rsutdio](https://wilson-camargo-jr.github.io/img/null-values.jpg)  

Vemos que existem diversas colunas com ausência de informação em seus registros, por exemplo, na coluna Gender, temos 489 registros com valores _“Male”_, 112  com _“Female”_ e 13 com indicação de informações ausentes.

Temos então que decidir o que fazer com os valores ausentes em cada coluna. Poderíamos simplesmente excluir as linhas que possuem colunas NA, ou preencher os ausentes com algum valor que seja mais apropriado, como por exemplo, a média aritmética das observações da coluna, o valor mais frequente, etc. 

A análise dever ser feita caso a caso, lembrando sempre que em qualquer das hipóteses de remoção ou substituição existe a possibilidade de estarmos inserindo algum componente de erro em nosso modelo.

a) Lidando com valores não disponíveis (NA)

Vamos carregar nossos datasets “train” e “test” em variáveis locais para realizar os devidos ajustes. Insira o seguinte código no **editor de programas** e execute os comandos:

```
# Carregando a biblioteca ORE  
library(ORE)  

# Conectando ao banco de dados.   
ore.connect("RQUSER", password="oracle", conn_string="", all=TRUE)   

# Carregando tabela para variavel de trabalho local  
training_set <- ore.pull(train)  
predict_set  <- ore.pull(test)  
```

Para executar o código no editor marque as linhas a serem executadas e clique em **Run** conforme figura abaixo.

![rsutdio](https://wilson-camargo-jr.github.io/img/run-code.jpg)

Ignore as mensagens de warning quando referentes a unique key:

\> # Carregando a biblioteca ORE   
\> library(ORE)   
\>    
\> # Conectando ao banco de dados.   
\> ore.connect("RQUSER", password="oracle", conn_string="", all=TRUE)   
\>    
\> # Carregando tabela para variavel de trabalho local   
\> training_set <- ore.pull(train)   
<span style="color:red">Warning message: ORE object has no unique key - using random order</span>  
\> predict_set  <- ore.pull(test)  
<span style="color:red">Warning message: ORE object has no unique key - using random order</span>  
\>  

Vamos rever quais colunas apresentam valores nulos em seus registros.  Execute o seguinte comando na **console** do R studio.

```
colSums(is.na(training_set))
colSums(is.na(predict_set))
```

![rsutdio](https://wilson-camargo-jr.github.io/img/null-values2.jpg)
 
 
Nossa estratégia (puramente para fins didáticos) será a seguinte:

1)	Gender:		substituir “NA” por “Male”
2)	Married:		substituir “NA” por “Yes”
3)	Dependents:		substituir “NA” por “0”
4)	Self_employed: 	substituir “NA” por “No”
5)	LoanAmount: 	substituir “NA” pela média aritmética dos valores
6)	LoanAmount_Term:  substituir “NA” pela mediana dos valores
7)	Credit_History:	substituir “NA” por 1


Então vamos acrescentar ao nosso código as seguintes linha:

# Ajustando “NA” no training_set 
# 
# Trocando valores nulos da coluna Gender por "Male". 
training_set$Gender[is.na(training_set$Gender)] <- "Male"  

# Trocando valores nulos da coluna Married por "Yes". 
training_set$Married[is.na(training_set$Married)] <- "Yes"  

# Trocando valores nulos da coluna Dependents por "0". 
training_set$Dependents[is.na(training_set$Dependents)] <- "0"  

# Trocando valores nulos da coluna Self_Employes por "No". 
training_set$Self_Employed[is.na(training_set$Self_Employed)] <- "No"  

# Trocando valores nulos da coluna LoanAmount pela media dos valores. 
training_set$LoanAmount <- ifelse(is.na(training_set$LoanAmount),                                      mean(training_set$LoanAmount, na.rm = TRUE),                                    training_set$LoanAmount)  

# Trocando valores nulos da coluna Loan_Amount_Term pela mediana dos valores. 
training_set$Loan_Amount_Term <- ifelse(is.na(training_set$Loan_Amount_Term),                                         median(training_set$Loan_Amount_Term,na.rm = TRUE),                                         training_set$Loan_Amount_Term)  

# Trocando valores nulos da coluna Credit_History por 1. 
training_set$Credit_History[is.na(training_set$Credit_History)] <-1 







Vamos verificar como ficou o dataset . Execute o seguinte comando na console.

colSums(is.na(training_set))

> colSums(is.na(training_set))
          Loan_ID            Gender           Married        Dependents                  
                0                 0                 0                 0
        Education     Self_Employed   ApplicantIncome CoapplicantIncome                    
                0                 0                 0                 0         
       LoanAmount  Loan_Amount_Term    Credit_History     Property_Area                   
                0                 0                 0                 0        
      Loan_Status                  
                0  
>

Então vamos repetir o mesmo procedimento para o dataset que nomeamos “predict_set”.  Assim, insira o seguinte código no editor e execute.


# Ajustando “NA” no predict_set 
# 
# Trocando valores nulos da coluna Gender por "Male". 
predict_set$Gender[is.na(predict_set$Gender)] <- "Male"  

# Trocando valores nulos da coluna Married por "Yes". 
predict_set$Married[is.na(predict_set$Married)] <- "Yes"  

# Trocando valores nulos da coluna Dependents por "0". 
predict_set$Dependents[is.na(predict_set$Dependents)] <- "0"  

# Trocando valores nulos da coluna Self_Employes por "No". 
predict_set$Self_Employed[is.na(predict_set$Self_Employed)] <- "No"  

# Trocando valores nulos da coluna LoanAmount pela media dos valores. 
predict_set$LoanAmount <- ifelse(is.na(predict_set$LoanAmount),                                      mean(predict_set$LoanAmount, na.rm = TRUE),                                    predict_set$LoanAmount)  

# Trocando valores nulos da coluna Loan_Amount_Term pela mediana dos valores. 
predict_set$Loan_Amount_Term <- ifelse(is.na(predict_set$Loan_Amount_Term),                                         median(predict_set$Loan_Amount_Term, na.rm = TRUE),                                         predict_set$Loan_Amount_Term)  

# Trocando valores nulos da coluna Credit_History por 1. 
predict_set$Credit_History[is.na(predict_set$Credit_History)] <-1 



b) Convertendo as variáveis categóricas

Em nosso dataset temos diversas variáveis não numéricas, dentre elas “Gender”,  “Married”,  “Dependents”, etc. 

Para poderem servir de insumo no modelo ANN, essas variáveis precisam ser convertidas para uma notação binária, de modo a não causar incoerências nem tendências no modelo. Além disso vamos normalizar os valores numéricos para evitar que as dimensões provoquem anomalias em nosso modelo.

Para isso utilizaremos os seguintes comandos:
 
# 
# Codificando as colunas com fatores do training_set (usando biblioteca caret): 
# 
library(caret) 
library(DMwR) 
# 
# Loan_Status = Y significa que o cliente pagou o emprestimo 
# 
dmy_train <- dummyVars(~ Gender + Married + Dependents + Education +                         Self_Employed + Credit_History + Property_Area + Loan_Status,  data = training_set)  

dmy_train_set <- data.frame(predict(dmy_train, newdata = training_set))  

# Removendo as colunas codificadas redundantes para evitar a "Armadilha da Variavel Dummy"
 
dmy_train_red <- dmy_train_set[c(1,3,5,6,7,9,11,13,14,15,17)]  
# 
# Removendo as colunas nao utilizadas (redundantes e desnecessarias) do training_set 
# 
val_train_set <- training_set[c(7,8,9,10)]  

# Ajustando a escala e centrando ao redor da media 
val_train_centered <- scale(val_train_set, center = TRUE, scale = TRUE)  

# Unindo os datasets de treino formando o dataset de aprendizado 
learn_set <- cbind(val_train_centered, dmy_train_red)   

# 
# Executando o mesmo procedimento para o predict_set 
# 
dmy_predict <- dummyVars(~ Gender + Married + Dependents + Education +                           Self_Employed + Credit_History + Property_Area,  data = predict_set)  
dmy_predict_set <- data.frame(predict(dmy_predict, newdata = predict_set)) 
dmy_predict_red <- dmy_predict_set[c(1,3,5,6,7,9,11,13,14,15)] 
val_predict_set <- predict_set[c(7,8,9,10)] 
val_predict_centered <- scale(val_predict_set, center = TRUE, scale = TRUE) 

# Unindo os datasets de previsão
 predict <- cbind(val_predict_centered,dmy_predict_red) 


Finalmente vamos dividir nosso dataset com uma parte para a fase de treino e outra para teste de nosso modelo.

# Dividindo o dataset de aprendizado em dataset de treino e teste 
library(caTools) 
set.seed(123) 
split <- sample.split(learn_set$Loan_Status.N, SplitRatio = 0.80) 

train_set <- subset(learn_set, split == TRUE) 
test_set  <- subset(learn_set, split == FALSE)  

x_train_set <- train_set[,-15] 
y_train_set <- train_set[,15]  
x_test_set <- test_set[,-15] 
y_test_set <- test_set[,15]  

x_train_set <- as.matrix(x_train_set, nrow=c(nrow(x_train_set)), ncol=15) 
x_test_set  <- as.matrix(x_test_set, nrow=c(nrow(x_test_set)), ncol=15) 
 

Setting up a Neural Network Model with Keras
 
1	First, we will create our neural network model by creating a new instance of a model object.
2	Next step is to add layers to the neural network by calling model.add method and passing in the type of layer.
3	The final step of defining a model is to compile it where Keras actually uses the TensorFlow at the back-end to build the model. At this stage, Keras needs two important inputs, type of loss function and optimization algorithm. The loss function is the measure of the accuracy of each prediction made by the model during the training process. An optimization algorithm is a method to find the best parameters of a neural network for which loss function is zero. 
4	We are ready with the neural network model. We use model.fit function to train the model with passing the training data and the expected output. When training completes, Keras will report the training accuracy of the model.
5	After training our model, we will go for the testing phase. The testing shall be done by calling model.evaluate function and passing in the testing data set and the expected output.
6	The trained model can be saved to a file by model.save function and requires the file name. 
7	In order to use the trained model, we will first load it by load model function and pass in a file name. Now we will call the predict function and pass in the new data.



<div id="disqus_thread"></div>
<script>
    
    
    var disqus_config = function () {
        // Replace PAGE_URL with your page's canonical URL variable
        this.page.url = 'https://wilson-camargo-jr.github.io/2018-06-28-ANN-ORE-P7';  
        
        // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        this.page.identifier = '2018-06-28-ANN-ORE-P7'; 
    };
    

    
    (function() {  // REQUIRED CONFIGURATION VARIABLE: EDIT THE SHORTNAME BELOW
        var d = document, s = d.createElement('script');
        
        // IMPORTANT: Replace EXAMPLE with your forum shortname!
        s.src = 'https://wilson-camargo-jr.disqus.com/embed.js';
        
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>
    Please enable JavaScript to view the 
    <a href="https://disqus.com/?ref_noscript" rel="nofollow">
        comments powered by Disqus.
    </a>
</noscript>
